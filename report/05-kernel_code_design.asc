== Kernel code design

The output of the planning code can be seen as of type `std::vector<std::vector<Chunk>>`: the external array's size is
the  effective number of used threads while the inner array holds the definition of the chunks to be worked on by each
thread to fully process the image. Each `Chunk` is a structure having a type, position and size.

NOTE: C++ constructs like `std::vector` or OpenCV's `cv::Mat` allow us to safely allocate memory on the heap without
      bothering about its deallocation later on.

=== The outer loop

While the first array is _O(1)_ indexed by `omp_get_thread_num()`, we have a loop in each thread iterating over its
chunks and selecting the right kernel to process each one by its type.

.process_parallel.cpp
[source,c++]
----
template<typename Operator, typename OpArgs = typename KernelParams<Operator>::type>
void processParallel(parallel::Plan &plan, OpArgs &params) {    <1>
    const auto &alloc = plan.effectiveRegions().allocation();

    using namespace morph::kern;

#pragma omp parallel default(none) shared(alloc, params) num_threads(alloc.size())
    {
        int core = omp_get_thread_num();

        for (const auto &ch : alloc[core]) {
            switch (ch.type) {                                  <2>
                case CHUNK_REGULAR:
                    // Use regular non-checking kernel here
                    _kernel_single<Operator>(params, ch);
                    break;
                default:
                    // Use "safe" kernel for image borders
                    _kernel_single_safe<Operator>(params, ch);
                    break;
            }
        }
    }
}
----
<1> `OpArgs` is a template parameter representing the arguments struct for a given morphological operator.
<2> We will introduce other branches when we will talk about <<Improved edge processing,Improved edge processing>>.

For example, `OpArgs` kernel arguments for the base operators (dilation and erosion) are defined by:

.operator_types.h
[source,c++]
----
// [...]

struct BaseOpParams {
    cv::Mat &out;           <1>
    cv::Mat const &src;
    StrEl &strEl;           <2>
};

// [...]
----
<1> By reference to avoid copies
<2> Not const algorithm input to allow direct pointer access

=== Defining kernels

To avoid code duplication, all the kernels are generated from the same source file, parameterized by the use of
preprocessor defines. This way we can `#ifndef` border checks in regular kernels or use different subsets of code for
each operator.

The source is then included in `kernels.h` as inline functions, this suggests the compiler to "paste" their code at the
call site instead of performing a regular function call at runtime. This is a further optimization at the expense of
a potentially increased binary file size.

.kernels.h
[source,c++]
----
#define KERN_METHOD_DEF(var) \
    template<typename Operator> \
    inline void _kernel_##var(typename KernelParams<Operator>::type &args, const parallel::Chunk &ch)

#define KERN_METHOD(op, var) \
    template<> inline void _kernel_##var<op>(typename KernelParams<op>::type &args, const parallel::Chunk &ch)

namespace morph {
    namespace kern {

        // Non-SIMD (single-pixel) kernels
        KERN_METHOD_DEF(single);                <1>

        // "safe" kernels with border checks
        KERN_METHOD_DEF(single_safe);

        KERN_METHOD(Dilate, single) {
#define K_METHOD_DILATE
#include "kern_sched_cpu.inl"
        }

        KERN_METHOD(Dilate, single_safe) {
#define K_METHOD_DILATE                         <2>
#define K_ENABLE_BORDER_CHECKS
#include "kern_sched_cpu.inl"
        }

        KERN_METHOD(Erode, single) {
#define K_METHOD_ERODE
#include "kern_sched_cpu.inl"
        }

        KERN_METHOD(Erode, single_safe) {
#define K_METHOD_ERODE
#define K_ENABLE_BORDER_CHECKS
#include "kern_sched_cpu.inl"
        }

    }
}
----
<1> `KERN_METHOD_DEF` is required to allow template specialization by `KERN_METHOD`
<2> Preprocessor defines to alter the code generated from the included file
    (all variables are is `#undef` at the end of this file after each inclusion)

=== Performance impact of custom planning

At this point we need to check if our custom scheduling of the cores and the definition of specialized kernels
did actually provide a speedup over the image processing algorithms.

[width="80%", align="center"]
vegalite::vega/bench_comp_plan.json[]

[width="80%", align="center"]
vegalite::vega/bench_comp_plan_8k.json[]

A benchmark of the new code shows us an effective speedup of ~1.06 for regular images, with a non-linear increase
to ~1.17 for an 8K picture. These results were initially not satisfactory for the work required to obtain them, but
the codebase switch allowed us to obtain more control over the separate processing kernels, unlocking the possibility
to perform additional optimizations as explained in <<Profiling and further optimization,Profiling and Optimization>>,
whose results are represented here in yellow.

