:nofooter:
:imagesdir: images
:stem: latexmath
:source-highlighter: rouge
:toc: preamble

= Parallelizing Mathematical Morphology
D. Lietavec, L. Zanussi — Advanced Computer Architecture (2018-2019)

__
Our project for the course consisted in the creation and the subsequent parallelization of a set of Mathematical
Morphology operators — the result is a C++ codebase depending on OpenCV for image I/O and OpenMP, which was used
to manage the thread pool and initially plan the parallel execution of the kernel.
We will then discuss about switching to a custom planning algorithm, and observe the results.
__

include::01-morphology.asc[]

== Serial implementation

The first step was a serial implementation of the matlab algorithm for the dilation. Then, once implemented correctly, we made the erode code. Since the erosion share some core operations with the dilation, we created a core file to have both a cleaner code without too many repetitions and to optimize the program.

[source,c++]
----
cv::Mat morph::dilate(const cv::Mat &image, const StrEl &strEl, const int nThreads) {
    assert(image.type() == CV_8UC1);

    cv::Size imSize = image.size();
    cv::Mat output(imSize, CV_8UC1);

for (int y = 0; y < imSize.height; ++y) {
for (int x = 0; x < imSize.width; ++x) {

            int val = 0;

            for (int j = strEl.yMin(); j <= strEl.yMax(); ++j) {
                for (int i = strEl.xMin(); i <= strEl.xMax(); ++i) {
                    int u = x + i;
                    int v = y + j;

                    if (v < 0 || v >= imSize.height) continue;
                    if (u < 0 || u >= imSize.width) continue;
                    if (!strEl.isSet(j, i)) continue;

                    int m = image.at<uint8_t>(v, u) + strEl.at(j, i);
                    if (m > val) val = m;
                }
            }
            output.at<uint8_t>(y, x) = static_cast<uint8_t>(val < 0xFF ? val : 0xFF);
        }
    }

    return output;
}
----

== Parallelization with OpenMP

In order to make the parallel implementation with OpenMP, we used an OMP parallel directive with `collapse(2)` to iterate on both the axes of the image:

[source,c++]
----

#pragma omp parallel for num_threads(nThreads) collapse(2) default(none) shared(image, imSize, strEl, output)

----

To have full control on how OpenMP manages the variables, we set the default attribute to none, then we set our variables state to shared.

//Where needed, parallelization of these algorithms combined with the use of the right CPU features allows to achieve
//real-time HD video processing in less than 25ms.

...

include::06-custom_planning.asc[]
include::07-kernel_code_design.asc[]
include::08-simd_with_intrinsics.asc[]
include::09-profiling_and_optimization.asc[]

== Experimental results

...

=== Comparing compilers

Another experiment is to measure the performance of the binary produced by different toolchains. GCC and LLVM Clang
are free and open source compilers available for all major platforms, while Microsoft's optimizing compiler `cl` is
part of the Visual Studio toolchain.

All binaries were compiled in release mode optimized for maximum speed (`-O3` in GCC and Clang, `/O2` for cl).
All the tools were at their latest version at the time of this writing.

vegalite::vega/bench_compilers.json[]
vegalite::vega/bench_compilers_simd.json[]

It is interesting to note that Clang performs slightly better than GCC with no use of SIMD instructions, while GCC
outperforms it when SIMD is on. The trend remains with smaller images, where Clang's speedup in non-SIMD kernels
becomes more noticeable.

Even after additional research we weren't able to get faster builds from MSVC.

NOTE: MSVC's ABI is not compatible with GCC and Clang, dependencies like OpenCV need to be re-compiled for each ABI.

== Conclusion

// Cite morph, morphbench, use of environment variables and arguments, sane defaults

// Further improvements, using OpenCL/CUDA or porting to ARM, using NEON
