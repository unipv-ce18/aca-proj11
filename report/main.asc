:nofooter:
:imagesdir: images
:stem: latexmath
:source-highlighter: rouge
:toc: preamble

= Parallelizing Mathematical Morphology
D. Lietavec, L. Zanussi — Advanced Computer Architecture (2018-2019)

__
Our project for the course consisted in the creation and the subsequent parallelization of a set of Mathematical
Morphology operators — the result is a C++ codebase depending on OpenCV for image I/O and OpenMP, which was used
to manage the thread pool and initially plan the parallel execution of the kernel.
We will then discuss about switching to a custom planning algorithm, and observe the results.
__

include::01-morphology.asc[]
include::02-serial.asc[]
include::03-parallel_openmp.asc[]
include::04-custom_planning.asc[]
include::05-kernel_code_design.asc[]
include::06-simd_with_intrinsics.asc[]
include::07-profiling_and_optimization.asc[]
include::08-results.asc[]
include::09-cuda_approach.asc[]

== Conclusion

Our approach to create a custom schedule for the cores resulted successful, however this does not mean that the compiler
is capable of performing substantial optimizations on its own. Attempting to optimize a different algorithm may simply
end up in over-engineering (affecting the overall quality of the codebase) instead of a real optimization that could
have simply be done by an optimizing compiler.

The way forward for algorithms of this kind is to switch to a GPGPU paradigm by the means of OpenCL or CUDA — we must
however consider that the spin-up time of CUDA might be excessive and would negatively impact the performance when
working over single HD sized images and its use may be more appropriate when working over batches of images (or less,
very large pictures).

We still obtained terrific results by using vector SIMD instructions on the CPU, with the switch to SSE2 approximating
well the theoretical speedup we anticipated; we were surprised to see results of the same order of magnitude from the
ones measured using CUDA.

There are still some experiments that can be done: for example porting to ARM and measuring the speedup obtained by
parallelization on that platform; NEON (SIMD) instructions could be used and tested as well. Additionally, a comparison
between dynamic and static scheduling can also be made, where static scheduling performs better for workloads naturally
balanced among the cores.
