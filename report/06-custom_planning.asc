== Custom planning

At this point, we decided to manually partition the image in advance in an attempt to further speedup the algorithm.
Our main motivations were:

* Obtaining square-like partitions for any image aspect ratio to have an higher probability for a single core to access
  the same pixel in its execution plan, thus optimizing cache usage;

* Avoiding border-checking branches in the main loop whenever possible by using a different variant of the kernel to
  process what's outside the _safe area_ of the image;

* Keeping the advantage of the previous solution to assign the same workload to every core, in order to avoid the overhead of dynamic scheduling.

[[planning1]]
.Regions planned for 32 cores on a 640x360 image, 10px safe area padding in blue
image::06-planning1.png[Grid of regions and safe area,align="center"]

The number of pixels outside the safe area is the radius of the structural element in that direction â€” this is to
ensure we don't fall outside the image when we overlay the center of the structural element over _any_ pixel inside
the safe area to calculate the new value for that pixel.

However, this naive solution has still some disadvantages that we wanted to overcome, at least in part:

* We don't have a lower bound on region size: the spin-up time of the thread pool (or even cache conflicts) may nullify
  or even thwart to the advantages of parallelization for small-sized images where it is sufficient to use only the main
  thread;

* The need for the corner regions to treat unsafe areas as a special case leads to a non balanced load between the
  threads.

=== Fixed size chunking

The solution chosen for the first problem was to split the safe area into chunks of the same size; additional ill-sized
chunks are made from pixels outside the safe area.
Only then chunks are evenly allocated to the selected number of threads when excess cores are simply left unused.

If the size of the chunk is chosen wisely, a large amount of our I/O over the image will be memory aligned; allowing us
also to use faster SIMD instructions for the main kernel later on.

[[planning2]]
.Effective vertical subdivision of the previous image, resulting in 12 chunks on a row
image::06-planning2.png[Vertical subdivision of the image,align="center"]

NOTE: As an approximation, more complex kernels working on the borders of the image are considered to take a similar
      amount of time to the inner, simpler kernels.

To have an even workload among the threads, the number of chunks allocated on a row to each thread is given by:

\[ \text{columns per core} = \lceil \frac{\text{# chunks on a row}}{\text{regions on a row}} \rceil \]

=== Maximizing core utilization

The ceiling operation causes some cores to be left unallocated: e.g. given the image above with 12 chunks split over 8
cores, the formula gives 2 chunks per core, resulting in 2 excess cores for each row.

Our solution was to plan for additional rows when the number of excess cores is larger than the number of cores
effectively allocated on each row.

[[planning3]]
.Scheduling of an additional row to maximize utilization, new vertical region splits are in purple, old ones in gray
image::06-planning3.png[Greater horizontal subdivision to maximize core usage,align="center"]

In our example configuration we increased the number of rows from 4 to 5, effectively reducing the total number of
excess cores from 8 to 2.

It can be noted that the original "gray" regions are not used anymore: regions are now being split horizontally as
multiple of chunks of a fixed size and vertically by the effective chunk row count.

=== Chunk Allocation

Now that we have bounds for the chunks, we can start allocating them to the various threads in order. The height of
each chunk is fixed (when possible), bounded by the vertical region splits.

[[planning4]]
.Effective allocation for chunks of size 64x32, 32 threads, safe padding of 10px, colors represent different threads
image::06-planning4.png[Effective chunk allocation,align="center"]
