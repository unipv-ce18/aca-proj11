== An alternative approach: CUDA

CUDA is a proprietary GPGPU ecosystem (alternative to the open standard OpenCL) developed by NVIDIA to allow execution
of highly parallel, generic algorithms over a graphics processor. A CUDA device has a number of streaming
multiprocessors (SMs), each of these is capable to run a great number of threads in parallel (e.g. 1024) scheduled
in _warps_ of 32 threads.

This highly parallel ecosystem gave rise to a SIMT architecture (single instruction, multiple threads) where a single
instruction operates on the data of all the threads in a warp. If a branch is taken in one of the threads, parallelism
is disabled for the warp and each thread is executed serially. It is important to note that the CUDA architecture
does not support branch prediction and speculative execution.

The programmer uses CUDA by executing kernels over _grids_. The grid is an abstraction that allows CUDA to better fit
to the domain of the algorithm: it is a 1D/2D/3D matrix of blocks, where each block is again a matrix of threads.
An hard limit exists over the maximum number of threads in a block and all the threads in a block are executed by a
single multiprocessor.

=== Implementation

For our porting to CUDA, we decided to use a single kernel for all the image â€” a variant of our OpenMP algorithm, to
avoid multiple executions with different kernels.

We decided to make each CUDA thread work over a single pixel of the output image. Blocks are squared and sized at
runtime in order to maximize the number of threads inside (e.g. if the GPU supports 1024 threads per block, we make
32x32 blocks). In the end we chose a 2D grid with the size of the image over the size of a single block.

To obtain better measurements of execution times, we measured timings by using the device events provided by the
CUDA API.

=== Results

Due to the difference in architecture with a regular CPU, it is difficult to compute a precise theoretical speedup by
using CUDA; however we can compare the devices under test by their number of SMs and attempt to calculate a speedup.

[frame="topbot",options="header"]
|======================
| Device         | SM count | Processing time | Theoretical speedup | Effective speedup
| RTX 2070 SUPER | 40       | 373.5 ms        | 3.08                | 4.77
| GTX 1080 Ti    | 28       | 722.9 ms        | 2.15                | 2.46
| GTX 970        | 13       | 1783.5 ms       | -                   | -
|======================

The mispredictions of the theoretical speedup calculation indicate that factors like clock time and architectural
differences between the various generations of cards have a large impact over the effective speedup of the algorithm.

[width="80%", align="center"]
vegalite::vega/bench_cuda.json[]

The observed results are surprisingly comparable to the ~850ms we obtained on 24 AVX512 threads on a Google Cloud Xeon
CPU with custom scheduling and optimized kernels (or the ~1070ms on 16 AVX2 threads on a a Core i7-6900k); but we have
to consider that our CUDA implementation might have allowed for some more optimization, and not every algorithm can
easily make use of SIMD.


From these results we have also proven that the time to transfer the data between the main memory and the graphics
memory for processing highly depends on the version of the PCIe standard in use, the GTX 970 card above was mounted
on an old machine and its transfer times were effectively doubled (PCIe 2.0 vs PCIe 3.0).
